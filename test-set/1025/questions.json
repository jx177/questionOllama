[
    "How does the mapped tagset compare to the annotation scheme used in the existing UD Thai treebank, specifically regarding the decision not to use the ADJ tag?",
    "How are syllables generated automatically from Thai words, and is there ambiguity in this process, requiring a heuristic?",
    "Is it possible to combine syllable information with BERT models to potentially improve performance, and have any such experiments been conducted?",
    "Does the ORCHID corpus already provide word segmentation and train/dev/test splits, or were these created specifically for this work?",
    "Why was the JSON format chosen for the mapped ORCHID corpus in the Supplementary Material, instead of the more standard CoNLL-U format used by UD?",
    "How does the performance of POS tagging with UD-style tags compare to that with ORCHID tags, and which tagset is easier for taggers to learn?",
    "Given that the ORCHID tagset is more fine-grained than the UD tagset, does the loss of information during mapping significantly affect the performance of taggers?",
    "What was the motivation for converting the ORCHID tagset to the UD tagset, given that the conversion appears to be lossy?",
    "What specific changes were made during the conversion process from ORCHID to UD, and how were sub-types mapped to morphological features?",
    "How do the results of the systems tested with the original ORCHID tagset compare to those tested with the converted UD tagset?",
    "How does the conversion process used in this paper compare to the conversion used in the existing Thai UD treebank (Thai PUD)?",
    "What was the tag distribution before and after the conversion from ORCHID to UD?",
    "Why did multi-lingual BERT fail to improve performance, even on out-of-vocabulary words?",
    "Why did using a pre-trained model hurt performance compared to simpler models?",
    "What is the extent of the variation in the results due to factors like random initialization and test set split?",
    "Why does the syllable-based model perform as well as BERT on in-vocabulary words, according to the results presented in Table 4?"
]