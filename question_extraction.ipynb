{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f9fb6ae",
   "metadata": {},
   "source": [
    "# Ollama Introduction\n",
    "This little notebook is a brief introduction to Ollama, a tool for interacting with open LLMs deployed on Ollama server (the server can also run locally)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07177e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import Client\n",
    "\n",
    "# initialize the Ollama client with the specified host\n",
    "ollama_host = \"http://10.167.31.201:11434/\"\n",
    "client = Client(host=ollama_host)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d22889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model(model='test:latest', modified_at=datetime.datetime(2025, 5, 6, 10, 7, 2, 955865, tzinfo=TzInfo(UTC)), digest='cffa12cd509c35382b142562f4c786471a2f8f72044c490b85d47304f8b545e1', size=47415724883, details=ModelDetails(parent_model='', format='gguf', family='qwen2', families=['qwen2'], parameter_size='72.7B', quantization_level='Q4_K_M')), Model(model='qwen2.5:3b', modified_at=datetime.datetime(2025, 4, 28, 15, 39, 44, 731776, tzinfo=TzInfo(UTC)), digest='357c53fb659c5076de1d65ccb0b397446227b71a42be9d1603d46168015c9e4b', size=1929912432, details=ModelDetails(parent_model='', format='gguf', family='qwen2', families=['qwen2'], parameter_size='3.1B', quantization_level='Q4_K_M')), Model(model='qwen2.5:7b', modified_at=datetime.datetime(2025, 4, 28, 15, 39, 21, 364334, tzinfo=TzInfo(UTC)), digest='845dbda0ea48ed749caafd9e6037047aa19acfcfd82e704d7ca97d631a0b697e', size=4683087332, details=ModelDetails(parent_model='', format='gguf', family='qwen2', families=['qwen2'], parameter_size='7.6B', quantization_level='Q4_K_M')), Model(model='qwen2.5:14b', modified_at=datetime.datetime(2025, 4, 28, 15, 38, 52, 820793, tzinfo=TzInfo(UTC)), digest='7cdf5a0187d5c58cc5d369b255592f7841d1c4696d45a8c8a9489440385b22f6', size=8988124069, details=ModelDetails(parent_model='', format='gguf', family='qwen2', families=['qwen2'], parameter_size='14.8B', quantization_level='Q4_K_M')), Model(model='gemma3:27b', modified_at=datetime.datetime(2025, 3, 20, 12, 12, 56, 979965, tzinfo=TzInfo(UTC)), digest='30ddded7fba6d6f9c2f26661e2feba2d7a26a75e20a817538c41c3716d92609d', size=17396936887, details=ModelDetails(parent_model='', format='gguf', family='gemma3', families=['gemma3'], parameter_size='27.4B', quantization_level='Q4_K_M')), Model(model='llama3.2:3b-instruct-q4_K_M', modified_at=datetime.datetime(2025, 2, 22, 15, 6, 56, 192242, tzinfo=TzInfo(UTC)), digest='a80c4f17acd55265feec403c7aef86be0c25983ab279d83f3bcd3abbcb5b8b72', size=2019393189, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='3.2B', quantization_level='Q4_K_M')), Model(model='ilsp/meltemi-instruct-v1.5:latest', modified_at=datetime.datetime(2025, 2, 6, 7, 8, 36, 54063, tzinfo=TzInfo(UTC)), digest='7b3ff01317409d5d641c5ea865ee88ed34a68c9e8f1aea8ccd52d7e50e435924', size=5634098905, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='7.5B', quantization_level='Q5_1')), Model(model='command-r7b:latest', modified_at=datetime.datetime(2025, 1, 28, 15, 55, 12, 19456, tzinfo=TzInfo(UTC)), digest='ff4e9696ef9f19b62e3f7d7261c95dcc9bb15a7c0398493366d851119fe2e1ef', size=5057031198, details=ModelDetails(parent_model='', format='gguf', family='cohere2', families=['cohere2'], parameter_size='8.0B', quantization_level='Q4_K_M')), Model(model='dolphin3:latest', modified_at=datetime.datetime(2025, 1, 28, 15, 51, 14, 694557, tzinfo=TzInfo(UTC)), digest='d5ab9ae8e1f22619a6be52e5694df422b7183a3883990a000188c363781ecb78', size=4920757726, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='8.0B', quantization_level='Q4_K_M')), Model(model='llama3.3:latest', modified_at=datetime.datetime(2025, 1, 28, 15, 44, 12, 895628, tzinfo=TzInfo(UTC)), digest='a6eb4748fd2990ad2952b2335a95a7f952d1a06119a0aa6a2df6cd052a93a3fa', size=42520413916, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='70.6B', quantization_level='Q4_K_M')), Model(model='deepseek-r1:70b', modified_at=datetime.datetime(2025, 1, 21, 8, 19, 21, 619342, tzinfo=TzInfo(UTC)), digest='0c1615a8ca32ef41e433aa420558b4685f9fc7f3fd74119860a8e2e389cd7942', size=42520397704, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='70.6B', quantization_level='Q4_K_M')), Model(model='deepseek-r1:14b', modified_at=datetime.datetime(2025, 1, 21, 6, 7, 59, 934715, tzinfo=TzInfo(UTC)), digest='ea35dfe18182f635ee2b214ea30b7520fe1ada68da018f8b395b444b662d4f1a', size=8988112040, details=ModelDetails(parent_model='', format='gguf', family='qwen2', families=['qwen2'], parameter_size='14.8B', quantization_level='Q4_K_M')), Model(model='phi4:latest', modified_at=datetime.datetime(2025, 1, 15, 20, 51, 1, 793997, tzinfo=TzInfo(UTC)), digest='ac896e5b8b34a1f4efa7b14d7520725140d5512484457fab45d2a4ea14c69dba', size=9053116391, details=ModelDetails(parent_model='', format='gguf', family='phi3', families=['phi3'], parameter_size='14.7B', quantization_level='Q4_K_M')), Model(model='granite3-moe:3b', modified_at=datetime.datetime(2025, 1, 3, 16, 53, 49, 758215, tzinfo=TzInfo(UTC)), digest='157f538ae66e0663cee4fb30ef11e3ccf1ae3c1bf6da2698dbac1df3a25698c7', size=2059368194, details=ModelDetails(parent_model='', format='gguf', family='granitemoe', families=['granitemoe'], parameter_size='3.4B', quantization_level='Q4_K_M')), Model(model='granite-embedding:278m-fp16', modified_at=datetime.datetime(2025, 1, 3, 10, 37, 25, 756835, tzinfo=TzInfo(UTC)), digest='1a37926bf842899cbf90583e3932f3820548716d9d07661ba622199ffe95c552', size=562777301, details=ModelDetails(parent_model='', format='gguf', family='bert', families=['bert'], parameter_size='277.45M', quantization_level='F16')), Model(model='granite3.1-moe:3b-instruct-q4_K_M', modified_at=datetime.datetime(2025, 1, 3, 10, 36, 37, 85043, tzinfo=TzInfo(UTC)), digest='df6f6578dba8595a1d834879f9ade35413748595139c5b6554cd264078f02c58', size=2016901724, details=ModelDetails(parent_model='', format='gguf', family='granitemoe', families=['granitemoe'], parameter_size='3.3B', quantization_level='Q4_K_M')), Model(model='qwen2.5:32b', modified_at=datetime.datetime(2024, 12, 19, 7, 58, 12, 403233, tzinfo=TzInfo(UTC)), digest='9f13ba1299afea09d9a956fc6a85becc99115a6d596fae201a5487a03bdc4368', size=19851349669, details=ModelDetails(parent_model='', format='gguf', family='qwen2', families=['qwen2'], parameter_size='32.8B', quantization_level='Q4_K_M')), Model(model='qwen2.5:72b', modified_at=datetime.datetime(2024, 12, 3, 22, 16, 32, 286271, tzinfo=TzInfo(UTC)), digest='424bad2cc13f72f5bcd8085de63f48e7ef2ae5bf50a30be4b3ade7d3258f5796', size=47415724625, details=ModelDetails(parent_model='', format='gguf', family='qwen2', families=['qwen2'], parameter_size='72.7B', quantization_level='Q4_K_M')), Model(model='llama3.2:latest', modified_at=datetime.datetime(2024, 9, 30, 13, 12, 8, 4728, tzinfo=TzInfo(UTC)), digest='a80c4f17acd55265feec403c7aef86be0c25983ab279d83f3bcd3abbcb5b8b72', size=2019393189, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='3.2B', quantization_level='Q4_K_M')), Model(model='gemma2:latest', modified_at=datetime.datetime(2024, 9, 24, 12, 56, 44, 426602, tzinfo=TzInfo(UTC)), digest='ff02c3702f322b9e075e9568332d96c0a7028002f1a5a056e0a6784320a4db0b', size=5443152417, details=ModelDetails(parent_model='', format='gguf', family='gemma2', families=['gemma2'], parameter_size='9.2B', quantization_level='Q4_0')), Model(model='llama3.1:latest', modified_at=datetime.datetime(2024, 9, 24, 12, 55, 33, 314457, tzinfo=TzInfo(UTC)), digest='42182419e9508c30c4b1fe55015f06b65f4ca4b9e28a744be55008d21998a093', size=4661230766, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='8.0B', quantization_level='Q4_0'))]\n"
     ]
    }
   ],
   "source": [
    "# Get a list of all models that are currently downloaded\n",
    "models = client.list()\n",
    "print(models.models)\n",
    "\n",
    "MODEL_NAME = \"llama3.1:70b\"\n",
    "\n",
    "# !ollama pull {MODEL_NAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917954bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "\n",
    "class QuestionResponse(BaseModel):\n",
    "    questions: List[str]\n",
    "\n",
    "def get_questions_from_review(review: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extracts scientific questions from a peer review.\n",
    "    \"\"\"\n",
    "    response = client.chat(\n",
    "        model=MODEL_NAME,\n",
    "        messages=[\n",
    "            {\"role\": \"user\",\n",
    "            \"content\": (\n",
    "                \"You are given a peer review of a scientific article.\\n\\n\"\n",
    "                \"Your task is to extract all **scientific, information-seeking questions** that the reviewer asked.\\n\\n\"\n",
    "\n",
    "                \"These questions should:\\n\"\n",
    "                \"- Seek clarification or justification about methods, data, results etc \\n\"\n",
    "                \"- Be grounded in the article’s scientific content\\n\"\n",
    "                \"- Be answerable by the authors with more scientific explanation or evidence\\n\\n\"\n",
    "                \"- Be explicitly posed by the reviewer, not implied or inferred. Not a comment. Must be a question.\\n\"\n",
    "\n",
    "                \"**DO NOT include** questions that are:\\n\"\n",
    "                \"- Editorial such as grammar, spelling, formatting, or structure\\n\"\n",
    "                \"- Rhetorical, evaluative, or based on the reviewer’s opinions \\n\"\n",
    "                \"- Referring to figures(e.g., “Figure 2, Figs. 3”)\\n\\n\"\n",
    "\n",
    "                \"Rephrase valid questions to be self-contained and precise. Each question should focus on a single scientific subject. Add any missing context that is necessary to make the question understandable on its own. Remove reference to any mention of line numbers, paragraph numbers, table numbers, or figure numbers. (e.g., 'L12', 'lines 300-309', 'paragraph 3', 'Table 2'). If the question becomes incomplete without the reference, DISCARD the whole question. \\n\\n\"\n",
    "\n",
    "                \"### Example 1:\\n\"\n",
    "                \"Input: \\\"In Table 2, he authors claim the signal-to-noise ratio improved significantly , but they don’t explain how it was measured. Was this ratio calculated over multiple trials or just a single run?\\\"\\n\"\n",
    "                \"Output: How was the signal-to-noise ratio measured in the modified setup—over multiple trials or a single run?\\n\\n\"\n",
    "                \"### Example 2:\\n\"\n",
    "                \"Input: \\\"In line 32-35, why is f(\\\\nu_1) being biquadrate exponential distribution function, etc\\\"\\n\"\n",
    "                \"Output: What is the reasoning behind choosing a biquadrate exponential distribution function for f(ν₁)?\\n\\n\"\n",
    "                \"### Example 3:\\n\"\n",
    "                \"Input: \\\"It is important to demonstrate this phenomenon occuring in vivo using primary T cells. The manuscript over interprets some of the data. \\\"\\n\"\n",
    "                \"Output: [] (No scientific, information-seeking question found.) \\n\\n\"\n",
    "                \"### Example 4:\\n\"\n",
    "                \"Input: \\\"L300-309. This seems to be a footnote, or possibly, an endnote. Please be clear about how it links to the other material.\\\"\\n\"\n",
    "                \"Output: [] (No scientific, information-seeking question found.) \\n\\n\"\n",
    "                \n",
    "                f\"Now apply the same process to the following review:\\n\\n{review}\"\n",
    "            )\n",
    "            }\n",
    "        ],\n",
    "        options={\n",
    "            \"temperature\": 0.4,\n",
    "            \"num_predict\": 1024,\n",
    "        },\n",
    "        format=QuestionResponse.model_json_schema()\n",
    "    )\n",
    "    structured_response = QuestionResponse.model_validate_json(response.message.content)\n",
    "    return structured_response.questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "482d4f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing directory: test-set\\1-10\n"
     ]
    },
    {
     "ename": "ResponseError",
     "evalue": "model \"llama3.1:8b\" not found, try pulling it first (status code: 404)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResponseError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Process each review\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m review \u001b[38;5;129;01min\u001b[39;00m reviews:\n\u001b[1;32m---> 22\u001b[0m     questions \u001b[38;5;241m=\u001b[39m \u001b[43mget_questions_from_review\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreview\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m     result\u001b[38;5;241m.\u001b[39mextend(questions)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Save the results \u001b[39;00m\n",
      "Cell \u001b[1;32mIn[60], line 13\u001b[0m, in \u001b[0;36mget_questions_from_review\u001b[1;34m(review)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_questions_from_review\u001b[39m(review: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m     10\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03m    Extracts scientific questions from a peer review.\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMODEL_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou are given a peer review of a scientific article.\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     19\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYour task is to extract all **scientific, information-seeking questions** that the reviewer asked.\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     20\u001b[0m \n\u001b[0;32m     21\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mThese questions should:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     22\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m- Seek clarification or justification about methods, data, results etc \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     23\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m- Be grounded in the article’s scientific content\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     24\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m- Be answerable by the authors with more scientific explanation or evidence\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     25\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m- Be explicitly posed by the reviewer, not implied or inferred. Not a comment. Must be a question.\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     26\u001b[0m \n\u001b[0;32m     27\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m**DO NOT include** questions that are:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     28\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m- Editorial such as grammar, spelling, formatting, or structure\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     29\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m- Rhetorical, evaluative, or based on the reviewer’s opinions \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     30\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m- Referring to figures(e.g., “Figure 2, Figs. 3”)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     31\u001b[0m \n\u001b[0;32m     32\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRephrase valid questions to be self-contained and precise. Each question should focus on a single scientific subject. Add any missing context that is necessary to make the question understandable on its own, but do not mention of line, paragraph, table, or figure numbers like L31, Table 2.\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     33\u001b[0m \n\u001b[0;32m     34\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m### Example 1:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     35\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mInput: \u001b[39;49m\u001b[38;5;130;43;01m\\\"\u001b[39;49;00m\u001b[38;5;124;43mIn Table 2, he authors claim the signal-to-noise ratio improved significantly , but they don’t explain how it was measured. Was this ratio calculated over multiple trials or just a single run?\u001b[39;49m\u001b[38;5;130;43;01m\\\"\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     36\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mOutput: How was the signal-to-noise ratio measured in the modified setup—over multiple trials or a single run?\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     37\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m### Example 2:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     38\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mInput: \u001b[39;49m\u001b[38;5;130;43;01m\\\"\u001b[39;49;00m\u001b[38;5;124;43mIn line 32-35, why is f(\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mnu_1) being biquadrate exponential distribution function, etc\u001b[39;49m\u001b[38;5;130;43;01m\\\"\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     39\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mOutput: What is the reasoning behind choosing a biquadrate exponential distribution function for f(ν₁)?\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     40\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m### Example 3:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     41\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mInput: \u001b[39;49m\u001b[38;5;130;43;01m\\\"\u001b[39;49;00m\u001b[38;5;124;43mIt is important to demonstrate this phenomenon occuring in vivo using primary T cells. The manuscript over interprets some of the data. \u001b[39;49m\u001b[38;5;130;43;01m\\\"\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     42\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mOutput: [] (No scientific, information-seeking question found.) \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     43\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m### Example 4:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     44\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mInput: \u001b[39;49m\u001b[38;5;130;43;01m\\\"\u001b[39;49;00m\u001b[38;5;124;43mL300-309. This seems to be a footnote, or possibly, an endnote. Please be clear about how it links to the other material.\u001b[39;49m\u001b[38;5;130;43;01m\\\"\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     45\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mOutput: [] (No scientific, information-seeking question found.) \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     46\u001b[0m \u001b[43m                \u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNow apply the same process to the following review:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mreview\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     48\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_predict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mQuestionResponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_json_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m     structured_response \u001b[38;5;241m=\u001b[39m QuestionResponse\u001b[38;5;241m.\u001b[39mmodel_validate_json(response\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent)\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m structured_response\u001b[38;5;241m.\u001b[39mquestions\n",
      "File \u001b[1;32mc:\\Users\\User\\miniconda3\\Lib\\site-packages\\ollama\\_client.py:342\u001b[0m, in \u001b[0;36mClient.chat\u001b[1;34m(self, model, messages, tools, stream, think, format, options, keep_alive)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchat\u001b[39m(\n\u001b[0;32m    298\u001b[0m   \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    299\u001b[0m   model: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    307\u001b[0m   keep_alive: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    308\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[ChatResponse, Iterator[ChatResponse]]:\n\u001b[0;32m    309\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;124;03m  Create a chat response using the requested model.\u001b[39;00m\n\u001b[0;32m    311\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;124;03m  Returns `ChatResponse` if `stream` is `False`, otherwise returns a `ChatResponse` generator.\u001b[39;00m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 342\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mChatResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/api/chat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatRequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_copy_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_copy_tools\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m      \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m      \u001b[49m\u001b[43mthink\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthink\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m      \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m      \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_dump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexclude_none\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\miniconda3\\Lib\\site-packages\\ollama\\_client.py:180\u001b[0m, in \u001b[0;36mClient._request\u001b[1;34m(self, cls, stream, *args, **kwargs)\u001b[0m\n\u001b[0;32m    176\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpart)\n\u001b[0;32m    178\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[1;32m--> 180\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mjson())\n",
      "File \u001b[1;32mc:\\Users\\User\\miniconda3\\Lib\\site-packages\\ollama\\_client.py:124\u001b[0m, in \u001b[0;36mClient._request_raw\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    122\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m r\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mHTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 124\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mtext, e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mConnectError:\n\u001b[0;32m    126\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(CONNECTION_ERROR_MESSAGE) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mResponseError\u001b[0m: model \"llama3.1:8b\" not found, try pulling it first (status code: 404)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "base_dir = 'test-set'\n",
    "\n",
    "for dir in os.listdir(base_dir):\n",
    "    dir_path = os.path.join(base_dir, dir)\n",
    "    if os.path.isdir(dir_path):\n",
    "        print(f\"Processing directory: {dir_path}\")\n",
    "\n",
    "        questions_json = os.path.join(dir_path, 'questions.json')\n",
    "        # if os.path.exists(questions_json):\n",
    "        #     print(f\"Questions file already exists: {questions_json}\")\n",
    "        #     continue\n",
    "\n",
    "        # Load the peer reviews from a JSON file\n",
    "        review_file = os.path.join(dir_path, 'all_reviews.json')\n",
    "\n",
    "        with open(review_file, 'r', encoding='utf-8') as file:\n",
    "            reviews = json.load(file)\n",
    "        \n",
    "        result = []\n",
    "        # Process each review\n",
    "        for review in reviews:\n",
    "            questions = get_questions_from_review(review)\n",
    "            result.extend(questions)\n",
    "\n",
    "        # Save the results \n",
    "        with open(questions_json, 'w', encoding='utf-8') as file:\n",
    "            json.dump(result, file, ensure_ascii=False, indent=4)\n",
    "        print(f\"Questions saved to {questions_json}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "457e680e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"What specific changes were made to address the reviewer's concern about the lack of chemistry in the manuscript?\",\n",
       " \"What was done to address the reviewer's concern about selectivity?\",\n",
       " 'What irrelevant information was removed from the manuscript?',\n",
       " \"What was added to the manuscript to fulfill the 'Data Availability' requirement?\",\n",
       " \"What was the primary concern of Reviewer 3 regarding the manuscript's suitability for a chemistry journal?\"]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_questions_from_review(review: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extracts scientific questions from a peer review.\n",
    "    \"\"\"\n",
    "    response = client.chat(\n",
    "        model=MODEL_NAME,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \n",
    "             \"content\": (\n",
    "                    \"You are given a text which contain peer review of a scientific article.\\n\\n\"\n",
    "                    \"Your task is to identify and extract all **scientific, information-seeking questions** posed by the reviewer.\\n\\n\"\n",
    "                    \"Only include questions related to the **scientific content** of the article — such as methodology, experimental design, data analysis, results, interpretation, assumptions, or scientific relevance.\\n\\n\"\n",
    "                    \"**Exclude** any questions about grammar, spelling, formatting, or writing style.\\n\\n\"\n",
    "                    \"If necessary, **rephrase the questions** so they are self-contained and preserve relevant scientific context. If a question is based on a specific sentence or result, include that sentence as context in the question.\\n\\n\"\n",
    "                    \"Return the extracted questions as JSON.\\n\\n\"\n",
    "                    f\"Peer review:\\n{review}\"\n",
    "            )}\n",
    "        ],\n",
    "        options={\n",
    "            \"temperature\": 0.5,\n",
    "            \"num_predict\": 1024,\n",
    "        },\n",
    "        format=QuestionResponse.model_json_schema()\n",
    "    )\n",
    "    structured_response = QuestionResponse.model_validate_json(response.message.content)\n",
    "    return structured_response.questions\n",
    "\n",
    "with open(\"all_reviews.json\", 'r', encoding='utf-8') as file:\n",
    "    reviews = json.load(file)\n",
    "get_questions_from_review(reviews[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7424c284",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f58951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As of 2023, the population of Paris is approximately **2.1 million** people.\n",
      "\n",
      "It’s important to note that this refers to the city proper (the administrative limits). The Greater Paris metropolitan area, which includes surrounding suburbs, has a much larger population – over 11 million! \n",
      "\n",
      "Would you like to know more about the population of Paris or its surrounding area?\n"
     ]
    }
   ],
   "source": [
    "# Send a simple prompt to the model\n",
    "# model: select a model from the list of models obtained from client.list()\n",
    "# messages: a list of messages containing the conversation history. Some models also \n",
    "# have a system message, to add this, make the first message:\n",
    "# {\"role\": \"system\", \"content\": \"Your system message here\"}\n",
    "# Gemma3 does not have a system message, so we can start with the user message.\n",
    "# To add responses from the model, you can use the \"assistant\" role, i.e.:\n",
    "# {\"role\": \"assistant\", \"content\": \"The capital of France is Paris.\"}\n",
    "\n",
    "response = client.chat(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"The capital of France is Paris.\"},\n",
    "        {\"role\": \"user\", \"content\": \"And who many people live there?\"},\n",
    "    ],\n",
    ")\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "537640a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the information-seeking questions from the list, excluding those referencing figures/lines/specific locations:\n",
      "\n",
      "*   **What is the reference source for the wave power formula used in the study?**\n",
      "*   **How does the unit of wave power translate to W/m (Watts per meter) from the provided formula, which is the product of squared significant height and wave period?**\n",
      "\n",
      "\n",
      "\n",
      "The other two questions specifically ask about elements *within* figures (Figure 2, Figures 2b,c,d,e) and are therefore excluded based on your criteria.\n"
     ]
    }
   ],
   "source": [
    "# To control the decoding parameters, such as temperature, maxium number of tokens, etc.,\n",
    "# you can pass additional parameters to the chat method.\n",
    "# For a complete list of options, check the Ollama API documentation at:\n",
    "# https://github.com/ollama/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\n",
    "x = [\n",
    "    \"How does the unit of wave power translate to W/m (Watts per meter) from the provided formula, which is the product of squared significant height and wave period?\",\n",
    "    \"What is the reference source for the wave power formula used in the study?\",\n",
    "    \"How can wave power be negative, given the observed range of -2000 to 2000 W/m in Figure 2?\",\n",
    "    \"Why does the latitude scale on the y-axis in Figures 2b, c, d, and e not maintain a fixed distance, as it does in Figure 2a?\",\n",
    "    \"What is the rationale behind the chosen color palette, where higher wave power and sea level are represented in reddish tones and lower values in bluish tones, while this representation is reversed for wave direction and waterline position?\",\n",
    "    \"What was the specific reason for removing 40% of transects from non-sandy beaches?\",\n",
    "    \"What is the unit of measurement for wave energy in Figure S3?\",\n",
    "    \"Why is the wave energy formula in Figure S3 the same as the wave power formula in Figure 2?\",\n",
    "    \"What was the basis for dividing the North American West Coast (NAWC) into five subregions?\",\n",
    "    \"What is the rationale for using a rectangular boundary in Figure 1?\",\n",
    "    \"Were any additional studies conducted to determine if parameters such as beach slope, substrate lithology,or riverine sediment inputs significantly affect waterline positions on a seasonal scale?\"]\n",
    "    \n",
    "response = client.chat(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \n",
    "         \"content\": \"From the following list of questions, extract only information seeking question and question with no reference to figrues, line etc : \"\n",
    "         \"How does the unit of wave power translate to W/m (Watts per meter) from the provided formula, which is the product of squared significant height and wave period?\"\n",
    "         \"What is the reference source for the wave power formula used in the study?\"\n",
    "         \"How can wave power be negative, given the observed range of -2000 to 2000 W/m in Figure 2?\"\n",
    "         \"Why does the latitude scale on the y-axis in Figures 2b, c, d, and e not maintain a fixed distance, as it does in Figure 2a?\"\n",
    "         },\n",
    "    ],\n",
    "    options={\n",
    "        \"temperature\": 0.5,\n",
    "        \"num_predict\": 1024,\n",
    "    }\n",
    ")\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61402533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "city='Paris' population=2141000\n"
     ]
    }
   ],
   "source": [
    "# To force the model to generate a structured response, i.e. a JSON object,\n",
    "# you can define a schema for the response and pass it. The schema can be defined using \n",
    "# Pydantic models and the built-in python types (more complex types are also supported, \n",
    "# check the pydantic documentation for more details).\n",
    "\n",
    "from pydantic import BaseModel\n",
    "class PopulationResponse(BaseModel):\n",
    "    city: str\n",
    "    population: int\n",
    "\n",
    "response = client.chat(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"The capital of France is Paris.\"},\n",
    "        {\"role\": \"user\", \"content\": \"And who many people live there?\"},\n",
    "    ],\n",
    "    format=PopulationResponse.model_json_schema()\n",
    ")\n",
    "structured_response = PopulationResponse.model_validate_json(response.message.content)\n",
    "print(structured_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d382277",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
