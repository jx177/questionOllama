{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f9fb6ae",
   "metadata": {},
   "source": [
    "# Ollama Introduction\n",
    "This little notebook is a brief introduction to Ollama, a tool for interacting with open LLMs deployed on Ollama server (the server can also run locally)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07177e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import Client\n",
    "\n",
    "# initialize the Ollama client with the specified host\n",
    "ollama_host = \"http://localhost:11434\"\n",
    "client = Client(host=ollama_host)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8d22889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model(model='gemma3:latest', modified_at=datetime.datetime(2025, 6, 11, 16, 51, 6, 220107, tzinfo=TzInfo(+02:00)), digest='a2af6cc3eb7fa8be8504abaf9b04e88f17a119ec3f04a3addf55f92841195f5a', size=3338801804, details=ModelDetails(parent_model='', format='gguf', family='gemma3', families=['gemma3'], parameter_size='4.3B', quantization_level='Q4_K_M')), Model(model='gemma3:27b', modified_at=datetime.datetime(2025, 6, 11, 16, 45, 59, 91280, tzinfo=TzInfo(+02:00)), digest='a418f5838eaf7fe2cfe0a3046c8384b68ba43a4435542c942f9db00a5f342203', size=17396936941, details=ModelDetails(parent_model='', format='gguf', family='gemma3', families=['gemma3'], parameter_size='27.4B', quantization_level='Q4_K_M'))]\n"
     ]
    }
   ],
   "source": [
    "# Get a list of all models that are currently downloaded\n",
    "models = client.list()\n",
    "print(models.models)\n",
    "\n",
    "MODEL_NAME = \"gemma3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "482d4f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing directory: test-set\\gmd-18-3311-2025\n",
      "Questions file already exists: test-set\\gmd-18-3311-2025\\questions_from_reviews.json\n",
      "Processing directory: test-set\\hgss-15-71-2024\n",
      "Questions file already exists: test-set\\hgss-15-71-2024\\questions_from_reviews.json\n",
      "Processing directory: test-set\\mr-6-119-2025\n",
      "Questions file already exists: test-set\\mr-6-119-2025\\questions_from_reviews.json\n",
      "Processing directory: test-set\\s42004-025-01575-2\n",
      "Questions saved to test-set\\s42004-025-01575-2\\questions_from_reviews.json\n",
      "Processing directory: test-set\\s43247-025-02414-x\n",
      "Questions saved to test-set\\s43247-025-02414-x\\questions_from_reviews.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "\n",
    "class QuestionResponse(BaseModel):\n",
    "    questions: List[str]\n",
    "\n",
    "def get_questions_from_review(review: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extracts scientific questions from a peer review.\n",
    "    \"\"\"\n",
    "    response = client.chat(\n",
    "        model=MODEL_NAME,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \n",
    "             \"content\": (\n",
    "                    \"You are given a peer review of a scientific article.\\n\\n\"\n",
    "                    \"Your task is to identify and extract all **scientific, information-seeking questions** posed by the reviewer.\\n\\n\"\n",
    "                    \"Only include questions related to the **scientific content** of the article — such as methodology, experimental design, data analysis, results, interpretation, assumptions, or scientific relevance.\\n\\n\"\n",
    "                    \"**Exclude** any questions about grammar, spelling, formatting, or writing style.\\n\\n\"\n",
    "                    \"If necessary, **rephrase the questions** so they are self-contained and independent of the original review context.\\n\\n\"\n",
    "                    \"Return the extracted questions as a list. \\n\\n\"\n",
    "                    f\"Peer review:\\n{review}\"\n",
    "            )}\n",
    "        ],\n",
    "        options={\n",
    "            \"temperature\": 0.8,\n",
    "            \"num_predict\": 1024,\n",
    "        },\n",
    "        format=QuestionResponse.model_json_schema()\n",
    "    )\n",
    "    structured_response = QuestionResponse.model_validate_json(response.message.content)\n",
    "    return structured_response.questions\n",
    "\n",
    "base_dir = 'test-set'\n",
    "\n",
    "for dir in os.listdir(base_dir)[2:]:\n",
    "    dir_path = os.path.join(base_dir, dir)\n",
    "    if os.path.isdir(dir_path):\n",
    "        print(f\"Processing directory: {dir_path}\")\n",
    "\n",
    "        questions_json = os.path.join(dir_path, 'questions_from_reviews.json')\n",
    "        if os.path.exists(questions_json):\n",
    "            print(f\"Questions file already exists: {questions_json}\")\n",
    "            continue\n",
    "\n",
    "        # Load the peer reviews from a JSON file\n",
    "        review_file = os.path.join(dir_path, 'all_reviews.json')\n",
    "\n",
    "        with open(review_file, 'r', encoding='utf-8') as file:\n",
    "            reviews = json.load(file)\n",
    "        \n",
    "        result = []\n",
    "        # Process each review\n",
    "        for review in reviews:\n",
    "            questions = get_questions_from_review(review)\n",
    "            result.extend(questions)\n",
    "\n",
    "        # Save the results \n",
    "        with open(questions_json, 'w', encoding='utf-8') as file:\n",
    "            json.dump(result, file, ensure_ascii=False, indent=4)\n",
    "        print(f\"Questions saved to {questions_json}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7424c284",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f58951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As of 2023, the population of Paris is approximately **2.1 million** people.\n",
      "\n",
      "It’s important to note that this refers to the city proper (the administrative limits). The Greater Paris metropolitan area, which includes surrounding suburbs, has a much larger population – over 11 million! \n",
      "\n",
      "Would you like to know more about the population of Paris or its surrounding area?\n"
     ]
    }
   ],
   "source": [
    "# Send a simple prompt to the model\n",
    "# model: select a model from the list of models obtained from client.list()\n",
    "# messages: a list of messages containing the conversation history. Some models also \n",
    "# have a system message, to add this, make the first message:\n",
    "# {\"role\": \"system\", \"content\": \"Your system message here\"}\n",
    "# Gemma3 does not have a system message, so we can start with the user message.\n",
    "# To add responses from the model, you can use the \"assistant\" role, i.e.:\n",
    "# {\"role\": \"assistant\", \"content\": \"The capital of France is Paris.\"}\n",
    "\n",
    "response = client.chat(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"The capital of France is Paris.\"},\n",
    "        {\"role\": \"user\", \"content\": \"And who many people live there?\"},\n",
    "    ],\n",
    ")\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "537640a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As of 2023, the population of Paris is approximately **2.1 million** people within the city limits. However, the greater Paris metropolitan area (which includes surrounding suburbs) has a population of over **11 million**! \n",
      "\n",
      "Would you like to know anything more about Paris?\n"
     ]
    }
   ],
   "source": [
    "# To control the decoding parameters, such as temperature, maxium number of tokens, etc.,\n",
    "# you can pass additional parameters to the chat method.\n",
    "# For a complete list of options, check the Ollama API documentation at:\n",
    "# https://github.com/ollama/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\n",
    "response = client.chat(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"The capital of France is Paris.\"},\n",
    "        {\"role\": \"user\", \"content\": \"And who many people live there?\"},\n",
    "    ],\n",
    "    options={\n",
    "        \"temperature\": 0.7,\n",
    "        \"num_predict\": 1024,\n",
    "    }\n",
    ")\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61402533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "city='Paris' population=2141636\n"
     ]
    }
   ],
   "source": [
    "# To force the model to generate a structured response, i.e. a JSON object,\n",
    "# you can define a schema for the response and pass it. The schema can be defined using \n",
    "# Pydantic models and the built-in python types (more complex types are also supported, \n",
    "# check the pydantic documentation for more details).\n",
    "\n",
    "from pydantic import BaseModel\n",
    "class PopulationResponse(BaseModel):\n",
    "    city: str\n",
    "    population: int\n",
    "\n",
    "response = client.chat(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"The capital of France is Paris.\"},\n",
    "        {\"role\": \"user\", \"content\": \"And who many people live there?\"},\n",
    "    ],\n",
    "    format=PopulationResponse.model_json_schema()\n",
    ")\n",
    "structured_response = PopulationResponse.model_validate_json(response.message.content)\n",
    "print(structured_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d382277",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
