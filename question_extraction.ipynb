{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f9fb6ae",
   "metadata": {},
   "source": [
    "# Ollama Introduction\n",
    "This little notebook is a brief introduction to Ollama, a tool for interacting with open LLMs deployed on Ollama server (the server can also run locally)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07177e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import Client\n",
    "\n",
    "# initialize the Ollama client with the specified host\n",
    "ollama_host = \"http://10.167.31.201:11434/\"\n",
    "# ollama_host = \"http://localhost:11434/\"\n",
    "client = Client(host=ollama_host)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e8d22889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model(model='test:latest', modified_at=datetime.datetime(2025, 5, 6, 10, 7, 2, 955865, tzinfo=TzInfo(UTC)), digest='cffa12cd509c35382b142562f4c786471a2f8f72044c490b85d47304f8b545e1', size=47415724883, details=ModelDetails(parent_model='', format='gguf', family='qwen2', families=['qwen2'], parameter_size='72.7B', quantization_level='Q4_K_M')), Model(model='qwen2.5:3b', modified_at=datetime.datetime(2025, 4, 28, 15, 39, 44, 731776, tzinfo=TzInfo(UTC)), digest='357c53fb659c5076de1d65ccb0b397446227b71a42be9d1603d46168015c9e4b', size=1929912432, details=ModelDetails(parent_model='', format='gguf', family='qwen2', families=['qwen2'], parameter_size='3.1B', quantization_level='Q4_K_M')), Model(model='qwen2.5:7b', modified_at=datetime.datetime(2025, 4, 28, 15, 39, 21, 364334, tzinfo=TzInfo(UTC)), digest='845dbda0ea48ed749caafd9e6037047aa19acfcfd82e704d7ca97d631a0b697e', size=4683087332, details=ModelDetails(parent_model='', format='gguf', family='qwen2', families=['qwen2'], parameter_size='7.6B', quantization_level='Q4_K_M')), Model(model='qwen2.5:14b', modified_at=datetime.datetime(2025, 4, 28, 15, 38, 52, 820793, tzinfo=TzInfo(UTC)), digest='7cdf5a0187d5c58cc5d369b255592f7841d1c4696d45a8c8a9489440385b22f6', size=8988124069, details=ModelDetails(parent_model='', format='gguf', family='qwen2', families=['qwen2'], parameter_size='14.8B', quantization_level='Q4_K_M')), Model(model='gemma3:27b', modified_at=datetime.datetime(2025, 3, 20, 12, 12, 56, 979965, tzinfo=TzInfo(UTC)), digest='30ddded7fba6d6f9c2f26661e2feba2d7a26a75e20a817538c41c3716d92609d', size=17396936887, details=ModelDetails(parent_model='', format='gguf', family='gemma3', families=['gemma3'], parameter_size='27.4B', quantization_level='Q4_K_M')), Model(model='llama3.2:3b-instruct-q4_K_M', modified_at=datetime.datetime(2025, 2, 22, 15, 6, 56, 192242, tzinfo=TzInfo(UTC)), digest='a80c4f17acd55265feec403c7aef86be0c25983ab279d83f3bcd3abbcb5b8b72', size=2019393189, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='3.2B', quantization_level='Q4_K_M')), Model(model='ilsp/meltemi-instruct-v1.5:latest', modified_at=datetime.datetime(2025, 2, 6, 7, 8, 36, 54063, tzinfo=TzInfo(UTC)), digest='7b3ff01317409d5d641c5ea865ee88ed34a68c9e8f1aea8ccd52d7e50e435924', size=5634098905, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='7.5B', quantization_level='Q5_1')), Model(model='command-r7b:latest', modified_at=datetime.datetime(2025, 1, 28, 15, 55, 12, 19456, tzinfo=TzInfo(UTC)), digest='ff4e9696ef9f19b62e3f7d7261c95dcc9bb15a7c0398493366d851119fe2e1ef', size=5057031198, details=ModelDetails(parent_model='', format='gguf', family='cohere2', families=['cohere2'], parameter_size='8.0B', quantization_level='Q4_K_M')), Model(model='dolphin3:latest', modified_at=datetime.datetime(2025, 1, 28, 15, 51, 14, 694557, tzinfo=TzInfo(UTC)), digest='d5ab9ae8e1f22619a6be52e5694df422b7183a3883990a000188c363781ecb78', size=4920757726, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='8.0B', quantization_level='Q4_K_M')), Model(model='llama3.3:latest', modified_at=datetime.datetime(2025, 1, 28, 15, 44, 12, 895628, tzinfo=TzInfo(UTC)), digest='a6eb4748fd2990ad2952b2335a95a7f952d1a06119a0aa6a2df6cd052a93a3fa', size=42520413916, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='70.6B', quantization_level='Q4_K_M')), Model(model='deepseek-r1:70b', modified_at=datetime.datetime(2025, 1, 21, 8, 19, 21, 619342, tzinfo=TzInfo(UTC)), digest='0c1615a8ca32ef41e433aa420558b4685f9fc7f3fd74119860a8e2e389cd7942', size=42520397704, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='70.6B', quantization_level='Q4_K_M')), Model(model='deepseek-r1:14b', modified_at=datetime.datetime(2025, 1, 21, 6, 7, 59, 934715, tzinfo=TzInfo(UTC)), digest='ea35dfe18182f635ee2b214ea30b7520fe1ada68da018f8b395b444b662d4f1a', size=8988112040, details=ModelDetails(parent_model='', format='gguf', family='qwen2', families=['qwen2'], parameter_size='14.8B', quantization_level='Q4_K_M')), Model(model='phi4:latest', modified_at=datetime.datetime(2025, 1, 15, 20, 51, 1, 793997, tzinfo=TzInfo(UTC)), digest='ac896e5b8b34a1f4efa7b14d7520725140d5512484457fab45d2a4ea14c69dba', size=9053116391, details=ModelDetails(parent_model='', format='gguf', family='phi3', families=['phi3'], parameter_size='14.7B', quantization_level='Q4_K_M')), Model(model='granite3-moe:3b', modified_at=datetime.datetime(2025, 1, 3, 16, 53, 49, 758215, tzinfo=TzInfo(UTC)), digest='157f538ae66e0663cee4fb30ef11e3ccf1ae3c1bf6da2698dbac1df3a25698c7', size=2059368194, details=ModelDetails(parent_model='', format='gguf', family='granitemoe', families=['granitemoe'], parameter_size='3.4B', quantization_level='Q4_K_M')), Model(model='granite-embedding:278m-fp16', modified_at=datetime.datetime(2025, 1, 3, 10, 37, 25, 756835, tzinfo=TzInfo(UTC)), digest='1a37926bf842899cbf90583e3932f3820548716d9d07661ba622199ffe95c552', size=562777301, details=ModelDetails(parent_model='', format='gguf', family='bert', families=['bert'], parameter_size='277.45M', quantization_level='F16')), Model(model='granite3.1-moe:3b-instruct-q4_K_M', modified_at=datetime.datetime(2025, 1, 3, 10, 36, 37, 85043, tzinfo=TzInfo(UTC)), digest='df6f6578dba8595a1d834879f9ade35413748595139c5b6554cd264078f02c58', size=2016901724, details=ModelDetails(parent_model='', format='gguf', family='granitemoe', families=['granitemoe'], parameter_size='3.3B', quantization_level='Q4_K_M')), Model(model='qwen2.5:32b', modified_at=datetime.datetime(2024, 12, 19, 7, 58, 12, 403233, tzinfo=TzInfo(UTC)), digest='9f13ba1299afea09d9a956fc6a85becc99115a6d596fae201a5487a03bdc4368', size=19851349669, details=ModelDetails(parent_model='', format='gguf', family='qwen2', families=['qwen2'], parameter_size='32.8B', quantization_level='Q4_K_M')), Model(model='qwen2.5:72b', modified_at=datetime.datetime(2024, 12, 3, 22, 16, 32, 286271, tzinfo=TzInfo(UTC)), digest='424bad2cc13f72f5bcd8085de63f48e7ef2ae5bf50a30be4b3ade7d3258f5796', size=47415724625, details=ModelDetails(parent_model='', format='gguf', family='qwen2', families=['qwen2'], parameter_size='72.7B', quantization_level='Q4_K_M')), Model(model='llama3.2:latest', modified_at=datetime.datetime(2024, 9, 30, 13, 12, 8, 4728, tzinfo=TzInfo(UTC)), digest='a80c4f17acd55265feec403c7aef86be0c25983ab279d83f3bcd3abbcb5b8b72', size=2019393189, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='3.2B', quantization_level='Q4_K_M')), Model(model='gemma2:latest', modified_at=datetime.datetime(2024, 9, 24, 12, 56, 44, 426602, tzinfo=TzInfo(UTC)), digest='ff02c3702f322b9e075e9568332d96c0a7028002f1a5a056e0a6784320a4db0b', size=5443152417, details=ModelDetails(parent_model='', format='gguf', family='gemma2', families=['gemma2'], parameter_size='9.2B', quantization_level='Q4_0')), Model(model='llama3.1:latest', modified_at=datetime.datetime(2024, 9, 24, 12, 55, 33, 314457, tzinfo=TzInfo(UTC)), digest='42182419e9508c30c4b1fe55015f06b65f4ca4b9e28a744be55008d21998a093', size=4661230766, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='8.0B', quantization_level='Q4_0'))]\n"
     ]
    }
   ],
   "source": [
    "# Get a list of all models that are currently downloaded\n",
    "models = client.list()\n",
    "print(models.models)\n",
    "\n",
    "MODEL_NAME = \"gemma3:27b\"\n",
    "#MODEL_NAME = \"llama3.3:latest\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55b3c23",
   "metadata": {},
   "source": [
    "# Question extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917954bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "def clean_text_for_model(text):\n",
    "    # 1. Remove Unicode replacement characters (�)\n",
    "    text = text.replace('\\uFFFD', '')\n",
    "    \n",
    "    # 2. Normalize Unicode (removes weird byte leftovers)\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    \n",
    "    # 3. Remove non-ASCII characters, but keep newlines\n",
    "    text = re.sub(r'[^\\x20-\\x7E\\n]+', '', text)\n",
    "    \n",
    "    # 4. Collapse excessive whitespace\n",
    "    text = re.sub(r'[ \\t]{2,}', ' ', text)  # multiple spaces/tabs → one space\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)  # 3+ newlines → 2\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "class QuestionResponse(BaseModel):\n",
    "    questions: List[str]\n",
    "\n",
    "def get_questions_from_review(review: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extracts scientific questions from a peer review.\n",
    "    \"\"\"\n",
    "    response = client.chat(\n",
    "        model=MODEL_NAME,\n",
    "        messages=[\n",
    "            {\"role\": \"user\",\n",
    "            \"content\": (\n",
    "                \"You are given a peer review of a scientific article.\\n\\n\"\n",
    "                \"Your task is to extract all **scientific, information-seeking questions** that the reviewer asked. It might contain no valid question, return empty list if that's the case. \\n\\n\"\n",
    "\n",
    "                \"These questions should:\\n\"\n",
    "                \"- Be explicitly posed by the reviewer, not implied or inferred from a statement. Must be a question.\\n\"\n",
    "                \"- Seek clarification or justification about methods, data, results etc \\n\"\n",
    "                \"- Be grounded in the article’s scientific content\\n\"\n",
    "                \"- Be answerable by the authors with more scientific explanation or evidence\\n\\n\"\n",
    "\n",
    "                \"**DO NOT include** questions or comments that are:\\n\"\n",
    "                \"- Editorial such as grammar, spelling, formatting, or structure\\n\"\n",
    "                \"- Rhetorical, evaluative, or based on the reviewer’s opinions \\n\"\n",
    "                \"- Referring to figures(e.g., “Figure 2, Figs. 3”)\\n\\n\"\n",
    "\n",
    "                \"Rephrase valid questions to be self-contained and precise. Each question should focus on a single scientific subject. Add any missing context that is necessary to make the question understandable on its own. \\n\\n\"\n",
    "\n",
    "                \"### Example 1:\\n\"\n",
    "                \"Input: \\\"In Table 2, the authors claim the signal-to-noise ratio improved significantly , but they don’t explain how it was measured. Was this ratio calculated over multiple trials or just a single run?\\\"\\n\"\n",
    "                \"Output: How was the signal-to-noise ratio measured in the modified setup—over multiple trials or a single run?\\n\\n\"\n",
    "                \"### Example 2:\\n\"\n",
    "                \"Input: \\\"In line 32-35, why is f(\\\\nu_1) being biquadrate exponential distribution function, etc\\\"\\n\"\n",
    "                \"Output: What is the reasoning behind choosing a biquadrate exponential distribution function for f(ν₁)?\\n\\n\"\n",
    "                \"### Example 3:\\n\"\n",
    "                \"Input: \\\"It is important to demonstrate this phenomenon occuring in vivo using primary T cells. The manuscript over interprets some of the data. \\\"\\n\"\n",
    "                \"Output: [] (No scientific, information-seeking question found.) \\n\\n\"\n",
    "                \"### Example 4:\\n\"\n",
    "                \"Input: \\\"L300-309.Please be clear about how it links to the other material. I also recommend adding a column detailing the quantum gate counts used in the ADAPT-VQE. \\\"\\n\"\n",
    "                \"Output: [] (No scientific, information-seeking question found.) \\n\\n\"\n",
    "                \"### Example 5:\\n\"\n",
    "                \"Input: \\\"The authors state that the results are statistically significant, but they do not provide the p-values. What are them?\\\"\\n\"\n",
    "                \"Output: What are the p-values for the key results presented in the study?\\n\\n\"\n",
    "                \n",
    "                f\"Now apply the same process to the following review:\\n\\n{review}\"\n",
    "            )\n",
    "            }\n",
    "        ],\n",
    "        options={\n",
    "            \"temperature\": 0.4,\n",
    "            \"num_predict\": 1024,\n",
    "        },\n",
    "        format=QuestionResponse.model_json_schema()\n",
    "    )\n",
    "\n",
    "    structured_response = QuestionResponse.model_validate_json(response.message.content)\n",
    "    return structured_response.questions\n",
    "\n",
    "def filter_questions_regex(questions: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Filters out any questions with figure, line and table reference.\n",
    "    \"\"\"\n",
    "    pattern = re.compile(\n",
    "        r'('\n",
    "        r'\\b(?i:'\n",
    "            r'\\bfig(?:ure)?(?:s)?(?:\\.)?(?:\\d+)?\\b'  # fig, figs, fig., figs., figure, figures + number, case-insensitive\n",
    "            r'|' \n",
    "            r'lines?\\s*\\d+(?:[-–]\\d+)?'             # line + number or number range (e.g., line 5 or line 5-10), case-insensitive\n",
    "            r'|'\n",
    "            r'table\\s*\\d+'                          # table + number, case-insensitive\n",
    "        r')'\n",
    "        r'|'  \n",
    "        r'\\bL\\d+(?:-\\d+)?'                          # Capital letter L + number or number range (e.g., L5 or L5-10), case-sensitive\n",
    "        r')\\b'\n",
    "    )\n",
    "\n",
    "    filtered_questions = [q for q in questions if not pattern.search(q)] \n",
    "    return filtered_questions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "482d4f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing directory: test-set\\1-10\n",
      "Questions file already exists: test-set\\1-10\\questions.json\n",
      "Processing directory: test-set\\1025\n",
      "Questions file already exists: test-set\\1025\\questions.json\n",
      "Processing directory: test-set\\gmd-18-3311-2025\n",
      "Questions file already exists: test-set\\gmd-18-3311-2025\\questions.json\n",
      "Processing directory: test-set\\hgss-15-71-2024\n",
      "Questions saved to test-set\\hgss-15-71-2024\\questions.json\n",
      "Processing directory: test-set\\mr-6-119-2025\n",
      "Questions file already exists: test-set\\mr-6-119-2025\\questions.json\n",
      "Processing directory: test-set\\s43247-025-02414-x\n",
      "Questions file already exists: test-set\\s43247-025-02414-x\\questions.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "base_dir = 'test-set'\n",
    "\n",
    "for dir in os.listdir(base_dir):\n",
    "    dir_path = os.path.join(base_dir, dir)\n",
    "    if os.path.isdir(dir_path):\n",
    "        print(f\"Processing directory: {dir_path}\")\n",
    "\n",
    "        questions_json = os.path.join(dir_path, 'questions.json')\n",
    "        if os.path.exists(questions_json):\n",
    "            print(f\"Questions file already exists: {questions_json}\")\n",
    "            continue\n",
    "\n",
    "        # Load the peer reviews from a JSON file\n",
    "        review_file = os.path.join(dir_path, 'all_reviews.json')\n",
    "\n",
    "        with open(review_file, 'r', encoding='utf-8') as file:\n",
    "            reviews = json.load(file)\n",
    "        \n",
    "        result = []\n",
    "        # Process each review\n",
    "        for review in reviews:\n",
    "            review = clean_text_for_model(review)\n",
    "            if not review.strip():\n",
    "                print(\"Empty review found, skipping.\")\n",
    "                continue\n",
    "            try:\n",
    "                questions = get_questions_from_review(review)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing review in {review_file}: {e}\")\n",
    "                # try again\n",
    "                try:\n",
    "                    questions = get_questions_from_review(review)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing review again in {review_file}: {e}\")\n",
    "                    questions = []\n",
    "            result.extend(questions)\n",
    "\n",
    "        if not result:\n",
    "            print(f\"No questions extracted from {review_file}\")\n",
    "            continue\n",
    "\n",
    "        result = list(set(result)) # Ensure uniqueness\n",
    "\n",
    "        # Filter the questions\n",
    "        filtered_questions = filter_questions_regex(result)\n",
    "        if filtered_questions is None:\n",
    "            print(f\"No questions left after filtering for {review_file}\")\n",
    "            continue\n",
    "\n",
    "        with open(questions_json, 'w', encoding='utf-8') as file:\n",
    "            json.dump(filtered_questions, file, ensure_ascii=False, indent=4)\n",
    "        print(f\"Questions saved to {questions_json}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e436d2",
   "metadata": {},
   "source": [
    "# Answer extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3780834c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import Dict\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "def clean_text_for_model(text):\n",
    "    # 1. Remove Unicode replacement characters (�)\n",
    "    text = text.replace('\\uFFFD', '')\n",
    "    \n",
    "    # 2. Normalize Unicode (removes weird byte leftovers)\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    \n",
    "    # 3. Remove non-ASCII characters, but keep newlines\n",
    "    text = re.sub(r'[^\\x20-\\x7E\\n]+', '', text)\n",
    "    \n",
    "    # 4. Collapse excessive whitespace\n",
    "    text = re.sub(r'\\n', ' ', text)  # newlines → space\n",
    "    text = re.sub(r'[ \\t]{2,}', ' ', text)  # multiple spaces/tabs → one space\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "class AnswerResponse(BaseModel):\n",
    "    QAPairs: Dict[str, str]\n",
    "\n",
    "def find_answer_in_rebuttal(question, rebuttal_text):\n",
    "    \"\"\"\n",
    "    Finds the answer to a given question within a rebuttal text.\n",
    "    If the rebuttal does not answer the question, return an empty string.\n",
    "    \"\"\"\n",
    "    response = client.chat(\n",
    "        model=MODEL_NAME,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": (\n",
    "                    f\"You are given a scientific question: {question}.\\n\\n\"\n",
    "                    f\"Your task is to find an answer to the given question from the following rebuttal. The rebuttal might contains both reviewer comments and author replies.\\n\\n\"\n",
    "                    \"Return only the answer in plain text. You may rephrase for better understanding. If no corresponding response is found, return 'none'. Do not repeat the question. Do NOT summarize the rebuttal.\\n\"\n",
    "\n",
    "                    f\"Rebuttal text:\\n{rebuttal_text}\\n\\n\"\n",
    "                    \"Answer:\"\n",
    "                )\n",
    "\n",
    "            }\n",
    "        ],\n",
    "        options={\n",
    "            \"temperature\": 0.2,\n",
    "            \"num_predict\": 512,\n",
    "        }\n",
    "    )\n",
    "    answer = response.message.content.strip()\n",
    "    # Clean the answer\n",
    "    if answer.lower() in [\"\", \"none\", \"n/a\"] or any(\n",
    "        phrase in answer.lower() for phrase in [\"the rebuttal does not contain\", \"no answer found\"]\n",
    "    ):\n",
    "        answer = None\n",
    "\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "114e794f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from textwrap import wrap\n",
    "import json\n",
    "import os\n",
    "\n",
    "# spacy.cli.download(\"en_core_web_md\")\n",
    "nlp = spacy.load(\"en_core_web_md\") \n",
    "\n",
    "MAX_CHARS = 2000\n",
    "\n",
    "def chunk_by_sentence_count(text, max_characters=MAX_CHARS):\n",
    "    doc = nlp(text)\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "\n",
    "    for sent in doc.sents:\n",
    "        sentence_text = sent.text.strip()\n",
    "        if current_chunk:\n",
    "            current_chunk += \" \" + sentence_text\n",
    "        else:\n",
    "            current_chunk = sentence_text\n",
    "\n",
    "        if len(current_chunk) >= max_characters:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = \"\"\n",
    "\n",
    "    # Add remaining chunk\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def extract_answer(paper_dir):\n",
    "    questions_json = os.path.join(paper_dir, 'questions.json')\n",
    "    with open(questions_json, 'r', encoding='utf-8') as file:\n",
    "        questions = json.load(file)\n",
    "\n",
    "    final_qa = {}\n",
    "\n",
    "    rebuttal_json = os.path.join(paper_dir, 'all_rebuttals.json')\n",
    "    with open(rebuttal_json, \"r\", encoding=\"utf-8\") as f:\n",
    "        rebuttals = json.load(f)\n",
    "\n",
    "    rebuttals_list = []\n",
    "    for r in rebuttals:\n",
    "        text = clean_text_for_model(r)\n",
    "        chunks = chunk_by_sentence_count(text)\n",
    "        rebuttals_list.append(chunks)\n",
    "        print(f\"Rebuttal split into {len(chunks)} chunks.\")\n",
    "\n",
    "        \n",
    "    for question in questions:\n",
    "        print(f\"Finding answer for question: {question}\")\n",
    "        \n",
    "        for rebuttal in rebuttals_list:\n",
    "            found = False\n",
    "            for chunk in rebuttal:\n",
    "                answer = find_answer_in_rebuttal(question, chunk)\n",
    "                if answer:\n",
    "                    print(f\"Answer found in chunk: {answer}\")\n",
    "                    final_qa[question] = answer\n",
    "                    found = True\n",
    "                    break  # Stop searching once we find an answer\n",
    "                print(f\"No answer found in chunk, moving to next chunk...\")\n",
    "            if found:\n",
    "                break\n",
    "                \n",
    "    # Save the final Q&A pairs to a JSON file\n",
    "    final_qa_json = os.path.join(paper_dir, 'final_qa.json')\n",
    "    with open(final_qa_json, 'w', encoding='utf-8') as file:\n",
    "        json.dump(final_qa, file, ensure_ascii=False, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6aae293d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing directory: test-set\\1-10\n",
      "Questions file or rebuttal file does not exist, skippingtest-set\\1-10.\n",
      "Processing directory: test-set\\1025\n",
      "Questions file or rebuttal file does not exist, skippingtest-set\\1025.\n",
      "Processing directory: test-set\\gmd-18-3311-2025\n",
      "Questions file or rebuttal file does not exist, skippingtest-set\\gmd-18-3311-2025.\n",
      "Processing directory: test-set\\hgss-15-71-2024\n",
      "Rebuttal split into 3 chunks.\n",
      "Rebuttal split into 2 chunks.\n",
      "Finding answer for question: What are the key issues related to distributed authorship in online encyclopedias, compared to the authorship of earlier encyclopedias?\n",
      "No answer found in chunk, moving to next chunk...\n",
      "No answer found in chunk, moving to next chunk...\n",
      "Answer found in chunk: Online encyclopedias, like Wikipedia, have a distributed authorship, which is different from earlier encyclopedias where authors were known. This distributed authorship brings issues such as articles being poor or dominated by the erroneous views of non-expert individuals.\n",
      "Finding answer for question: What was the reasoning for singling out thunderstorm scientists as having incomplete understanding of physical phenomena?\n",
      "Answer found in chunk: Are all details of physical phenomena ever completely understood? (This implies a questioning of whether any scientists, including those studying thunderstorms, have *complete* understanding, rather than singling them out specifically.)\n",
      "Finding answer for question: What is the connection between the material in lines 300-309 and the rest of the article?\n",
      "Answer found in chunk: The material in lines 300-309 (now Chapter 1) provides the motivation for the work – studying the development of descriptions and explanations of lightning and thunder over time using encyclopedias as a source – and outlines the structure of the article by detailing what each chapter will examine chronologically.\n",
      "Processing directory: test-set\\mr-6-119-2025\n",
      "Rebuttal split into 18 chunks.\n",
      "Finding answer for question: Would one expect the observed enhanced magnetization decay at rotary-resonance conditions in a truly liquid-like sample, such as glycine in water, where molecular tumbling occurs on the picosecond timescale?\n",
      "Answer found in chunk: The rebuttal states that rotary-resonance conditions arise due to the periodic dependence, and are not due to the amplitude or width of the distribution (either external magnetic field or radio-frequency field irradiation). While these factors influence the width, modulation frequency, and minimum intensity of the spin-lock signal, the core phenomenon isn't dependent on them. This implies that such conditions would be expected even in a truly liquid-like sample with picosecond timescale tumbling.\n",
      "Finding answer for question: Is there a justification for summing over n=1,2 in Eq S3, or is this motivated by experimental observations?\n",
      "Answer found in chunk: The summation over n=1,2 in Eq S3 is motivated by the modeling of the spatial rf-field distribution using distribution functions f_B1 and f_nu1, which are used to compute the total signal as a weighted average.\n",
      "Finding answer for question: What is the rationale for restricting the study to polyethylene glycol (PEG) and rubber, rather than other materials?\n",
      "No answer found in chunk, moving to next chunk...\n",
      "Answer found in chunk: The text does not contain an answer to the question about the rationale for restricting the study to polyethylene glycol (PEG) and rubber.\n",
      "Finding answer for question: What is the minimum amplitude of radiofrequency (RF) modulations, relative to the nominal RF amplitude, needed to observe the reported phenomenon?\n",
      "Answer found in chunk: The amplitude and width of the distribution influence the width of the rotary-resonance conditions, the modulation frequency, and the minimum intensity of the spin-lock signal at these conditions, but the rotary-resonance conditions arise due to the periodic dependence.\n",
      "Finding answer for question: Is it sufficient to consider inhomogeneity distribution along one axis in both scenarios, and if so, why?\n",
      "Answer found in chunk: The rebuttal states that considering the inhomogeneity distribution along one axis (the rotor axis) is sufficient because the rotary-resonance conditions arise due to the periodic dependence, not the amplitude or width of the distribution. While amplitude and width influence the width, modulation frequency, and minimum intensity of the spin-lock signal, the fundamental conditions are based on the periodic dependence.\n",
      "Finding answer for question: What is the experimental radiofrequency (rf) field inhomogeneity?\n",
      "Answer found in chunk: The spatial radiofrequency (rf) field inhomogeneity was modeled using distribution functions f_B1 (amplitude distribution along the rotor axis) and f_nu1 (magnitude of the modulated component along the y axis).\n",
      "Finding answer for question: What are the specific terms included in Eq S2 of the simulation details?\n",
      "Answer found in chunk: The terms included in Eq S2 are the initial density operator and the detection operator, both corresponding to Ix (x magnetization).\n",
      "Finding answer for question: What is the reasoning behind choosing a biquadrate exponential distribution function for f(ν₁)?\n",
      "Answer found in chunk: The authors model the spatial rf-field distribution using a distribution function f_nu1, which models the magnitude of the modulated component along the y-axis.\n",
      "Finding answer for question: What is the extent to which residual dipolar couplings might contribute to the observed enhanced relaxation in the rubber and polymer samples?\n",
      "Answer found in chunk: The rebuttal does not directly address the contribution of residual dipolar couplings to enhanced relaxation in rubber and polymer samples. Therefore, the answer is none.\n",
      "Finding answer for question: What is the molecular weight of the polyethylene glycol (PEG) used in the study?\n",
      "No answer found in chunk, moving to next chunk...\n",
      "No answer found in chunk, moving to next chunk...\n",
      "No answer found in chunk, moving to next chunk...\n",
      "No answer found in chunk, moving to next chunk...\n",
      "No answer found in chunk, moving to next chunk...\n",
      "No answer found in chunk, moving to next chunk...\n",
      "Answer found in chunk: The polyethylene glycol (PEG) molecular weight is not mentioned in the provided text.\n",
      "Finding answer for question: Where exactly does the second recoupling condition at two times ωR originate—from MAS modulations of residual dipolar couplings or from RF field modulation?\n",
      "Answer found in chunk: The appearance of rotary resonance conditions in isotropic samples is not due to the amplitude or width of the distribution (either external magnetic field or radio-frequency field irradiation), but rather due to the periodic dependence. While amplitude and width influence the width, modulation frequency, and minimum intensity of the spin-lock signal at these conditions.\n",
      "Processing directory: test-set\\s43247-025-02414-x\n",
      "Rebuttal split into 42 chunks.\n",
      "Finding answer for question: What is the reference for the wave power formula used in the study?\n",
      "No answer found in chunk, moving to next chunk...\n",
      "Answer found in chunk: The actual formula used to generate the data is Pwave = g [2] Hs [2] [T] [p] [{p][64] * []* [q][. The reference for this formula is [2].\n",
      "Finding answer for question: What specific climatological or geomorphological features define the lateral boundaries of the PNW, NCA, and other study areas?\n",
      "No answer found in chunk, moving to next chunk...\n",
      "No answer found in chunk, moving to next chunk...\n",
      "No answer found in chunk, moving to next chunk...\n",
      "Answer found in chunk: The Pacific Northwest corresponds to Washington, Oregon, and the northernmost part of California, while the boundaries between Southern California, Baja California, and Baja California Sur follow administrative borders. California was divided into two subregions based on distinct coastal conditions like coastline orientation, wave climate, and shoreline variability.\n",
      "Finding answer for question: Were any additional studies conducted to determine if parameters like beach slope, substrate lithology, or riverine sediment inputs significantly affect seasonal waterline positions?\n",
      "No answer found in chunk, moving to next chunk...\n",
      "No answer found in chunk, moving to next chunk...\n",
      "No answer found in chunk, moving to next chunk...\n",
      "No answer found in chunk, moving to next chunk...\n",
      "No answer found in chunk, moving to next chunk...\n",
      "No answer found in chunk, moving to next chunk...\n",
      "Answer found in chunk: The rebuttal does not directly address whether studies were conducted to determine if beach slope, substrate lithology, or riverine sediment inputs affect seasonal waterline positions. However, it states the authors revised the manuscript to include statistical relationships between key drivers (wave power and direction, and monthly-mean sea level) and waterline positions, suggesting an attempt to quantify relationships between drivers and waterline positions.\n",
      "Finding answer for question: What is the rationale behind the color palette used, where higher wave power and sea level are represented in reddish tones and lower values in bluish tones, while the representation is reversed for wave direction and waterline position?\n",
      "No answer found in chunk, moving to next chunk...\n",
      "Answer found in chunk: The representation of higher wave power and sea level in reddish tones and lower values in bluish tones, reversed for wave direction and waterline position, is a design choice with no specific rationale beyond that, though the authors acknowledge the inconsistency and state they have corrected the colorbar label to reflect \"demeaned wave power\".\n",
      "Finding answer for question: How is the unit of wave power translated to W/m from the formula provided, which is the product of squared significant height and wave period?\n",
      "No answer found in chunk, moving to next chunk...\n",
      "Answer found in chunk: The units are correctly W/m (watts per meter), though the formula described in the figure caption was incorrect. The actual formula used is Pwave = gH^2Tp, where g is gravity, H is significant wave height, T is wave period, and p is a coefficient.\n",
      "Finding answer for question: What was the basis for dividing the North American West Coast (NAWC) into five subregions?\n",
      "No answer found in chunk, moving to next chunk...\n",
      "No answer found in chunk, moving to next chunk...\n",
      "Answer found in chunk: The North American West Coast was divided into five subregions to highlight boundaries and improve readability of a figure, as indicated by the addition of horizontal lines to panels b-e.\n",
      "Finding answer for question: What specific criteria were used to justify the removal of 40% of transects from non-sandy beaches?\n",
      "No answer found in chunk, moving to next chunk...\n",
      "No answer found in chunk, moving to next chunk...\n",
      "Answer found in chunk: Non-sandy coasts were excluded to maintain consistency in the timescales analyzed, as they change more gradually over geological timescales while the study focuses on seasonal to interannual climate variability. Additionally, the detection methods were designed for sandy beaches, and non-sandy coasts exhibit lower rates of change, potentially leading to a high noise-to-signal ratio and reduced reliability.\n",
      "Finding answer for question: How can wave power be negative, given the observed range of -2000 to 2000 W/m?\n",
      "No answer found in chunk, moving to next chunk...\n",
      "Answer found in chunk: The displayed values of wave power, ranging from -2000 to +2000 W/m, represent de-meaned values of seasonal variability around the mean.\n",
      "Finding answer for question: How do geomorphic processes influence the response of waterlines to ENSO events?\n",
      "Answer found in chunk: The claims regarding ENSO and PDO events with waterline position are well explained.\n"
     ]
    }
   ],
   "source": [
    "base_dir = 'test-set'\n",
    "\n",
    "for dir in os.listdir(base_dir):\n",
    "    paper_dir = os.path.join(base_dir, dir)\n",
    "    if os.path.isdir(paper_dir):\n",
    "        print(f\"Processing directory: {paper_dir}\")\n",
    "        questions_json = os.path.join(paper_dir, 'questions.json')\n",
    "        rebutttal_json = os.path.join(paper_dir, 'all_rebuttals.json')\n",
    "        if not os.path.exists(questions_json) or not os.path.exists(rebutttal_json):\n",
    "            print(f\"Questions file or rebuttal file does not exist, skipping{paper_dir}.\")\n",
    "            continue\n",
    "        # Extract answers for the questions in the paper\n",
    "        extract_answer(paper_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7424c284",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f58951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Alright, let me break down what's happening here. The user previously asked about the capital of France and I told them it's Paris. Now they're following up with a question about how many people live there. \n",
      "\n",
      "Hmm, the user wrote \"who many people\" instead of \"how many.\" That's a common typo, so I should make sure to correct that in my response without pointing out the mistake explicitly.\n",
      "\n",
      "They’re probably looking for population statistics. Since Paris is a major city, they might be planning a trip, doing research, or just curious about its size relative to other cities. \n",
      "\n",
      "I remember that Paris has around 2.165 million people as of recent estimates. But it's also part of a larger metropolitan area called Île-de-France, which has over 12 million residents. That makes the metro area one of the largest in Europe.\n",
      "\n",
      "I should provide both numbers because the user might be interested in either the city proper or the broader area. It gives them a clearer picture depending on their needs.\n",
      "\n",
      "Also, I want to present this information clearly and concisely so they can easily understand the scale of Paris's population.\n",
      "</think>\n",
      "\n",
      "As of recent estimates, the city of Paris has a population of about **2.165 million people**. However, the larger metropolitan area of Paris (known as Île-de-France) is home to over **12 million residents**, making it one of the most populous urban regions in Europe.\n"
     ]
    }
   ],
   "source": [
    "# Send a simple prompt to the model\n",
    "# model: select a model from the list of models obtained from client.list()\n",
    "# messages: a list of messages containing the conversation history. Some models also \n",
    "# have a system message, to add this, make the first message:\n",
    "# {\"role\": \"system\", \"content\": \"Your system message here\"}\n",
    "# Gemma3 does not have a system message, so we can start with the user message.\n",
    "# To add responses from the model, you can use the \"assistant\" role, i.e.:\n",
    "# {\"role\": \"assistant\", \"content\": \"The capital of France is Paris.\"}\n",
    "\n",
    "response = client.chat(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"The capital of France is Paris.\"},\n",
    "        {\"role\": \"user\", \"content\": \"And who many people live there?\"},\n",
    "    ],\n",
    ")\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537640a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the information-seeking questions from the list, excluding those referencing figures/lines/specific locations:\n",
      "\n",
      "*   **What is the reference source for the wave power formula used in the study?**\n",
      "*   **How does the unit of wave power translate to W/m (Watts per meter) from the provided formula, which is the product of squared significant height and wave period?**\n",
      "\n",
      "\n",
      "\n",
      "The other two questions specifically ask about elements *within* figures (Figure 2, Figures 2b,c,d,e) and are therefore excluded based on your criteria.\n"
     ]
    }
   ],
   "source": [
    "# To control the decoding parameters, such as temperature, maxium number of tokens, etc.,\n",
    "# you can pass additional parameters to the chat method.\n",
    "# For a complete list of options, check the Ollama API documentation at:\n",
    "# https://github.com/ollama/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\n",
    "x = [\n",
    "    \"How does the unit of wave power translate to W/m (Watts per meter) from the provided formula, which is the product of squared significant height and wave period?\",\n",
    "    \"What is the reference source for the wave power formula used in the study?\",\n",
    "    \"How can wave power be negative, given the observed range of -2000 to 2000 W/m in Figure 2?\",\n",
    "    \"Why does the latitude scale on the y-axis in Figures 2b, c, d, and e not maintain a fixed distance, as it does in Figure 2a?\",\n",
    "    \"What is the rationale behind the chosen color palette, where higher wave power and sea level are represented in reddish tones and lower values in bluish tones, while this representation is reversed for wave direction and waterline position?\",\n",
    "    \"What was the specific reason for removing 40% of transects from non-sandy beaches?\",\n",
    "    \"What is the unit of measurement for wave energy in Figure S3?\",\n",
    "    \"Why is the wave energy formula in Figure S3 the same as the wave power formula in Figure 2?\",\n",
    "    \"What was the basis for dividing the North American West Coast (NAWC) into five subregions?\",\n",
    "    \"What is the rationale for using a rectangular boundary in Figure 1?\",\n",
    "    \"Were any additional studies conducted to determine if parameters such as beach slope, substrate lithology,or riverine sediment inputs significantly affect waterline positions on a seasonal scale?\"]\n",
    "    \n",
    "response = client.chat(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \n",
    "         \"content\": \"From the following list of questions, extract only information seeking question and question with no reference to figrues, line etc : \"\n",
    "         \"How does the unit of wave power translate to W/m (Watts per meter) from the provided formula, which is the product of squared significant height and wave period?\"\n",
    "         \"What is the reference source for the wave power formula used in the study?\"\n",
    "         \"How can wave power be negative, given the observed range of -2000 to 2000 W/m in Figure 2?\"\n",
    "         \"Why does the latitude scale on the y-axis in Figures 2b, c, d, and e not maintain a fixed distance, as it does in Figure 2a?\"\n",
    "         },\n",
    "    ],\n",
    "    options={\n",
    "        \"temperature\": 0.5,\n",
    "        \"num_predict\": 1024,\n",
    "    }\n",
    ")\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61402533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "city='Paris' population=2148000\n"
     ]
    }
   ],
   "source": [
    "# To force the model to generate a structured response, i.e. a JSON object,\n",
    "# you can define a schema for the response and pass it. The schema can be defined using \n",
    "# Pydantic models and the built-in python types (more complex types are also supported, \n",
    "# check the pydantic documentation for more details).\n",
    "\n",
    "from pydantic import BaseModel\n",
    "class PopulationResponse(BaseModel):\n",
    "    city: str\n",
    "    population: int\n",
    "\n",
    "response = client.chat(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"The capital of France is Paris.\"},\n",
    "        {\"role\": \"user\", \"content\": \"And who many people live there?\"},\n",
    "    ],\n",
    "    format=PopulationResponse.model_json_schema()\n",
    ")\n",
    "structured_response = PopulationResponse.model_validate_json(response.message.content)\n",
    "print(structured_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d382277",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = (\n",
    "    \"Identify and extract all **scientific, information-seeking questions** posed by the reviewer.\\n\\n\"\n",
    "    \"Each question must:\\n\"\n",
    "    \"- Seek clarification or justification about methods, data, assumptions, results, or interpretations\\n\"\n",
    "    \"- Be answerable by the authors with scientific explanation\\n\"\n",
    "    \"Do NOT include:\\n\"\n",
    "    \"- Editorial question about grammar, style, or formatting\\n\"\n",
    "    \"- Rhetorical comment\\n\"\n",
    "    \"- Question regarding figures\\n\"\n",
    "    \n",
    "    \"Rephrase the question to be self-contained and precise. Remove reference to any line or table number. If context loses after removal, discard the question.\\n\\n\"\n",
    "\n",
    "    \"### Examples:\\n\"\n",
    "    \"Input: 'In Table 2, the authors claim the signal-to-noise ratio improved. Was it over multiple trials or one run?'\\n\"\n",
    "    \"Output: [\\\"How was the signal-to-noise ratio measured in the modified setup—over multiple trials or a single run?\\\"]\\n\\n\"\n",
    "\n",
    "    \"Input: 'The manuscript overinterprets the data.'\\n\"\n",
    "    \"Output: []\\n\\n\"\n",
    "\n",
    "    \"Now extract the questions from this review:\\n\\n{REVIEW}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bd2321",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Error code: 404 - {'error': {'message': 'Your organization must be verified to use the model `o3-2025-04-16`. Please go to: https://platform.openai.com/settings/organization/general and click on Verify Organization. If you just verified, it can take up to 15 minutes for access to propagate.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mScientificQuestions\u001b[39;00m(BaseModel):\n\u001b[0;32m     13\u001b[0m     questions: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]\n\u001b[1;32m---> 15\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponses\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mo3-2025-04-16\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mExtract scientific questions from given peer reviews and return them as a structured list.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{REVIEW}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreview_text\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mScientificQuestions\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Access the parsed output\u001b[39;00m\n\u001b[0;32m     25\u001b[0m questions \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39moutput_parsed\n",
      "File \u001b[1;32mc:\\Users\\User\\miniconda3\\Lib\\site-packages\\openai\\resources\\responses\\responses.py:953\u001b[0m, in \u001b[0;36mResponses.parse\u001b[1;34m(self, input, model, text_format, tools, include, instructions, max_output_tokens, metadata, parallel_tool_calls, previous_response_id, reasoning, store, stream, temperature, text, tool_choice, top_p, truncation, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    946\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparser\u001b[39m(raw_response: Response) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ParsedResponse[TextFormatT]:\n\u001b[0;32m    947\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parse_response(\n\u001b[0;32m    948\u001b[0m         input_tools\u001b[38;5;241m=\u001b[39mtools,\n\u001b[0;32m    949\u001b[0m         text_format\u001b[38;5;241m=\u001b[39mtext_format,\n\u001b[0;32m    950\u001b[0m         response\u001b[38;5;241m=\u001b[39mraw_response,\n\u001b[0;32m    951\u001b[0m     )\n\u001b[1;32m--> 953\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    954\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/responses\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    955\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    956\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    957\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    958\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    959\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minclude\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minstructions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_output_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_output_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    962\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    963\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprevious_response_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprevious_response_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    965\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    967\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    968\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtruncation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    974\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    976\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mResponseCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    978\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    981\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    982\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    983\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    984\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    985\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# we turn the `Response` instance into a `ParsedResponse`\u001b[39;49;00m\n\u001b[0;32m    986\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# in the `parser` function above\u001b[39;49;00m\n\u001b[0;32m    987\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mType\u001b[49m\u001b[43m[\u001b[49m\u001b[43mParsedResponse\u001b[49m\u001b[43m[\u001b[49m\u001b[43mTextFormatT\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mResponse\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    988\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\miniconda3\\Lib\\site-packages\\openai\\_base_client.py:1249\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1235\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1236\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1237\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1244\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1245\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1246\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1247\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1248\u001b[0m     )\n\u001b[1;32m-> 1249\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\User\\miniconda3\\Lib\\site-packages\\openai\\_base_client.py:1037\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1034\u001b[0m             err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1036\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1037\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1039\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not resolve response (should never happen)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Error code: 404 - {'error': {'message': 'Your organization must be verified to use the model `o3-2025-04-16`. Please go to: https://platform.openai.com/settings/organization/general and click on Verify Organization. If you just verified, it can take up to 15 minutes for access to propagate.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "openai.api_key =\n",
    "\n",
    "review_text = \"I am now satisfied with the changes that have been made by the authors, and these revisions have made an improvement from the first version of the article. \\nThere are a few other minor concerns that the author may want to be consider; 1. All the data presented in this manuscript are generated by T cell lines or T cell \\u201cclone\\u201d. D10 cells are T lymphoblasts that continuously proliferate without stimulation, thereby exhibiting cancer cell properties. It is important to demonstrate this phenomenon occuring in vivo using primary T cells. \\n2. It isn\\u2019t very unclear how TCR internalization was measured by using anti-TCR antibody in the beginning and the end of incubation; this could be explained better. \\n3. In the abstract, it would read better if the word \\u201cinterestingly\\u201d was deleted from the start of the second paragraph. \"\n",
    "\n",
    "\n",
    "client = OpenAI(api_key=openai.api_key)\n",
    "\n",
    "class ScientificQuestions(BaseModel):\n",
    "    questions: list[str]\n",
    "\n",
    "response = client.responses.parse(\n",
    "    model=\"o3-2025-04-16\",\n",
    "    input=[\n",
    "        {\"role\": \"system\", \"content\": \"Extract scientific questions from given peer reviews and return them as a structured list.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt.replace(\"{REVIEW}\", review_text)}\n",
    "    ],\n",
    "    text_format=ScientificQuestions\n",
    ")\n",
    "\n",
    "# Access the parsed output\n",
    "questions = response.output_parsed\n",
    "print(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06f79a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
